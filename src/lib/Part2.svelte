<style>
    p {
        font-size: large;
    }

</style>

<p>
<strong>Transcript:</strong>
<br><br>
In the last video I started introducing predictive processing as a theory that views the brain as an inference machine, constantly engaging in predictions and revising predictive models in light of new evidence, with the best inferences determining the content of our perception. 

But how does the brain make good inferences?

<br><br>

The theory posits that the brain utilizes a statistical method using Bayesâ€™ theorem to update the probability of our hypotheses about causes, as more evidence becomes available. 

The key idea is that the best inference is one with the highest posterior probability. This is calculated by considering how likely it is that your hypothesis would produce the evidence you observe: e.g. If I was in a house and heard a muffled bang, if my hypothesis was that it was a bird hitting the window, the likelihood is the probability that a bird hitting the window would make that the sound that I heard. 

The likelihood on its own is not that informative. Because the same effect could have multiple different causes, in this instance the muffled sound could be exactly the sort of sound that a small UFO crashing into the house would sound like. 

<br><br>

So, the likelihood is considered alongside the probability of your hypothesis independently of the evidence. This is known as the prior. In this example the prior is the probability of a bird hitting my window at this time of day.

Using this information and applying Bayes theorem we can calculate the posterior probability of each hypothesis.

As you acquire more evidence e.g. Additional banging sounds of increasing frequency, hypotheses can be quickly re-ranked in order of posterior probability. In this case the posterior probability of the sound being caused by, my neighbour banging on the window would dramatically increase with each knock. 

</p>